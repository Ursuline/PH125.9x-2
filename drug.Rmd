---
title: "Predicting Cannabis Consumption from Demographics and Personality"
subtitle: "HarvardX PH125.9x Data Science Capstone"
author: "Charles MÃ©gnin"
date: "10/12/2019"
output:
  pdf_document:
    toc: yes
  html_document:
    number_sections: no
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = TRUE,
	cache = FALSE,
	tidy = TRUE
)
```

```{r load R packages}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(mlbench)) install.packages("mlbench", repos = "http://cran.us.r-project.org")
if(!require(grid)) install.packages("grid", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(gplots)) install.packages("gplots", repos = "http://cran.us.r-project.org")
if(!require(graphics)) install.packages("graphics", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org")

if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
load("drugEnvironment.RData")
```

# Executive Summary

## + Introduction

Drug use is a behavior that constitutes an important factor linked to poor health, including early mortality, and which presents significant adverse consequences for the social fabric, notably with respect to criminality and family cohesion. Early detection of an individual's predisposition to drug consumption offers healthcare professionals an opportunity to short-circuit the onset of addiction. 

The present study is based on a dataset that includes demographic and psychological information related to the consumption of 18 legal and illegal drugs by 1885 participants. For the purpose of this study, we choose to focus the data analysis and modeling on the use of cannabis.

## + Goal of project

The goal of this project is to assess whether an individual's consumption of cannabis can be predicted from a combination of demographic and personality data. 

To do so, we build and assess the effectiveness of six machine learning classifiers and confront the results obtained with the insights provided by data exploration.

## + Dataset description

The dataset used here is found on the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29).
It is based the research paper by E. Fehrman, A. K. Muhammad, E. M. Mirkes, V. Egan and A. N. Gorban, ["The Five Factor Model of personality and evaluation of drug consumption risk.," arXiv, 2015](https://arxiv.org/abs/1506.06297). The data was collected from 1885 English-speaking participants over 18 years of age between March 2011 and March 2012.

The original dataset includes answers to questions related to the use of alcohol, amphetamines, amyl nitrite, benzodiazepines, cannabis, chocolate, cocaine, caffeine, crack, ecstasy, heroin, ketamine, legal highs, LSD, methadone, magic mushrooms, nicotine and volatile substance abuse (VSA)) and one fictitious drug (Semeron) which was introduced to identify over-claimers. In the present study, we restrict our scope to the analysis of cannabis consumption.

In the original dataset, drug use is separated between 'Never used', 'Used over a decade ago', 'Used in last decade', 'Used in last year', 'Used in last month', 'Used in last week' and 'Used in last day'. For the purpose of this study, we separate the data in two groups: 'Non-Users' which is a combination of 'Never used', 'Used over a decade ago', 'Used in last decade', and 'Used' (the combination of the others, concisting of 'Used in last year', 'Used in last month', 'Used in last week' and 'Used in last day'). 

We create a classification that distinguishes those that have either never used cannabis or used it a decade ago or more and which we refer to as 'Non-users', from those that have used it more recently which we call 'Users'. This nomenclature is used for convenience, not for its negative connotation. We chose to bin the data this way as we find the ten-year mark to be a reasonable dividing line between those with no interest in cannabis use from those who occasionally or regularly consume it.

The features in the data can be separated in two groups of pre-normalized and centered predictors: 

1. Five demographic predictors : Age, Gender, Level of education, Ethnicity, and Country of origin.

2. The results from seven scored tests administered to assess personality, specifically:

  + Neuroticism (a long-term tendency to experience negative emotions such as nervousness, tension, anxiety and depression);
  + Extraversion (manifested in outgoing, warm, active, assertive, talkative, cheerful, and in search of stimulation characteristics);
  + Openness to experience (a general appreciation for art, unusual ideas, and imaginative, creative, unconventional, and wide interests);
  + Agreeableness (a dimension of interpersonal relations, characterized by altruism, trust, modesty, kindness, compassion and cooperativeness);
  + Conscientiousness (a tendency to be organized and dependable, strong-willed, persistent, reliable, and efficient);
  + Impulsiveness;
  + Sensation-seeking.

The working dataset in this study consists therefore of one Class (Cannabis consumption) and twelve predictors (five demographic and seven personality-related).

## + Key steps

We extract a training subset (80% of data) from the dataset for the purpose of training our model, and use the remaining 20% of the data as a test set for the purpose of evaluation. This being a classification problem, we use accuracy as the metric to assess the goodness of fit.

The analysis consists of two main sections:

1. In the first part, after performing minor data engineering (A), we bin, explore, and analyze the dataset (B). 
2. In the second part, we move on to the modeling phase (C):
  * After a 3-step preprocessing consisting of examining correlation among predictors, seeking low-variance factors and applying a Recursive Feature Elimination algorithm to seek and potentially discard predictors that do not contribute significantly to the outcome, we build models based on the following six popular machine learning methods:
    + Generalized linear model (glm)
    + Generalized linear model with penalized maximum likelihood (GLMnet)
    + Decision tree (rpart)
    + Random forest (rf)
    + Stochastic gradient boosting (gbm)
    + Neural network (nnet)

We compare the modeling approaches, both in terms of accuracy and coherence with the data analysis.

\newpage 

# Analysis

## A: Data engineering
* All predictors were already normalized and centered in the original dataset;
* We construct the 'Used' class to separate 'Non users' from 'Users';
* We partition the data between training (80% - `r nrow(df.train)` participants) and test sets (20% - `r nrow(df.test)` participants), preserving the distribution of the Cannabis class;
* There are `r sum(anyNA(df.raw))` NAs in the dataset as a whole (no interpolation or imputation to perform)

## B: Data exploration
```{r plot parameters, message=FALSE, warning=FALSE, paged.print=FALSE}
# Global plot parameters
fill <- 'skyblue3'
color <- 'grey'
fill_no <- "#af8dc3"
fill_yes <- "#7fbf7b"
used_colors <- c(fill_no, fill_yes) # No/Yes
alpha <- 0.4 # alpha-blending
axis_text_size <- 10
```

#### Class distribution
```{r overall consumption, message=FALSE, warning=FALSE, paged.print=FALSE}
plot.use
```

The training set of `r df.train %>% nrow()` participants consists of 
`r df.train %>% filter(Used == 1) %>% nrow()` 
participants (`r sprintf("%0.1f%%", round(df.train %>% filter(Used == 1) %>% nrow()/nrow(df.train)*100, digits = 1))`) having used cannabis and 
`r df.train %>% filter(Used == 0) %>% nrow()` who never have (`r sprintf("%0.1f%%", round(df.train %>% filter(Used == 0) %>% nrow()/nrow(df.train)*100, digits = 1))`), for a user-to-non-user ratio of 1:`r sprintf("%0.1f", round(df.train %>% filter(Used == 1) %>% nrow()/df.train %>% filter(Used == 0) %>% nrow(), digits = 1))`

### Contingency plots (prior to binning)

#### Cannabis use by demographic group
```{r contingency plots before binning, fig.height=5, fig.width=8, message=FALSE, paged.print=FALSE}
# Age contingency plot 
plot.balloon.age
# Gender contingency plot 
plot.balloon.gender
# Education contingency plot 
plot.balloon.edu
# Country contingency plot 
plot.balloon.country
# Ethnicity contingency plot 
plot.balloon.ethn
```

The dataset is dominated by young and educated white American and British participants of both sexes.

#### Binning
The small size of many demographic sub-groups add little valuable insight and will likely only serve to introduce variance in the analysis. 
At the risk of erasing behavioral differences among groups, the distribution of the dataset forces a more meaningful binning of the demographic information. In particular and in addition to the small individual population sizes, we can think of no rational reason to distinguish among those that left school before 16, at 16, at 17 or at 18 and lump these in a "Left school as a teen" group.

Visual inspection of the balloon plots above provide a straightforward path to  binning the data as:

- 5 age groups: "18-24", "25-34", "35-44", "45-54", "55+"
- 5 groups for Education: "Left school as a teen", "Some college", "Professional certificate", "University degree", "Masters degree" and "Doctorate".
- 3 groups for Country: "USA", "UK", "Others".
- 2 ethnic groups: "Whites", "Non-whites".

```{r rebin demographics, message=FALSE, paged.print=FALSE}
df.train <- 
  df.train %>% 
  mutate(Country = ifelse(Country %in% c(-0.09765, 0.24923, -0.46841, 0.21128), -0.28519, Country)) %>%
  mutate(Age = ifelse(Age == 2.59171, 1.82213, Age)) %>%
  mutate(Education = ifelse(Education %in% c(-2.43591, -1.73790, -1.43719), -1.22751, # Dropped school
                            ifelse(Education == 1.98437, 1.16365, Education))) %>% # Merge MS & PhD 
  mutate(Ethnicity = ifelse(Ethnicity != -0.31685, 0.11440, Ethnicity))

df.test <- 
  df.test %>% 
  mutate(Country = ifelse(Country %in% c(-0.09765, 0.24923, -0.46841, 0.21128), -0.28519, Country)) %>%
  mutate(Age = ifelse(Age == 2.59171, 1.82213, Age)) %>%
  mutate(Education = ifelse(Education %in% c(-2.43591, -1.73790, -1.43719), -1.22751, # Dropped school
                            ifelse(Education == 1.98437, 1.16365, Education))) %>% # Merge MS & PhD 
  mutate(Ethnicity = ifelse(Ethnicity != -0.31685, 0.11440, Ethnicity))
```

### Analysis of demographics
```{r contingency plot, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(plot.country, plot.gender, plot.ethn,
             plot.age, plot.edu,
             layout_matrix = rbind(c(1, 1, 2, 2, 3, 3), 
                                   c(4, 4, 4, 5, 5, 5)),
             top = "Use of cannabis in training set by:",
             left = "Counts")
```

Users outnumber non-users in many demographic sub-group, particularly for men, in the USA, among 18-24 year olds and for those that didn't complete a college degree. In the modeling phase, we therefore expect the corresponding predictors (country, age group, education and gender) to have significant weight.

```{r ratio plot, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(plot.prop.country, plot.prop.gender, plot.prop.ethn,
             plot.prop.age, plot.prop.edu,
             layout_matrix = rbind(c(1, 1, 2, 2, 3, 3), 
                                   c(4, 4, 4, 5, 5, 5)),
             top = "Ratio of Users to Non-Users by demographic group",
             left = "ratio(Users : Non-Users)")
```

For better readability of the differences among them, we use the ratio of users to non-users as index of consumption for each demographic sub-group.

* Americans consume more cannabis (`r sprintf("%0.1f", round((table.country %>% filter(Var == "USA"))$Prop, digits = 1))`) 
than all other nationalities, and particularly the British 
(`r sprintf("%0.1f", round((table.country %>% filter(Var == "UK"))$Prop, digits = 1))`) 
who, along with females 
(`r sprintf("%0.1f", round((table.gender %>% filter(Var == "female"))$Prop, digits = 1))`), 
people over 34 years of age and people with degrees, are among the lowest using group with more non-consumers than consumers.
* Men consume more 
(`r sprintf("%0.1f", round((table.gender %>% filter(Var == "male"))$Prop, digits = 1))`)
than women
(`r sprintf("%0.1f", round((table.gender %>% filter(Var == "female"))$Prop, digits = 1))`)
* Whites consume about the same as other ethnic groups 
(`r sprintf("%0.1f", round((table.ethn %>% filter(Var == "White"))$Prop, digits = 1))`)
and we do not expect ethnicity to be a significant factor.
(`r sprintf("%0.1f", round((table.ethn %>% filter(Var == "Non-white"))$Prop, digits = 1))`)
* Those having only had some college/university are the highest users in the education sub-group
(`r sprintf("%0.1f", round((table.edu %>% filter(Var == "Some college/univ."))$Prop, digits = 1))`)
* Finally, we observe a steady decline of cannabis use with age from 18-24 year olds
(`r sprintf("%0.1f", round((table.age %>% filter(Var == "18-24"))$Prop, digits = 1))`)
to participants over 55
(`r sprintf("%0.1f", round((table.age %>% filter(Var == "55+"))$Prop, digits = 1))`)
This points to a generational phenomenon (for instance, roughly five times as many 25-34 year-olds abstain when compared to 18-24 years olds, even though they have had ten additional years to experiment). 

\newpage
### Personality analysis

```{r personality box plot, fig.height=10, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(plot.box.Nscore + theme(legend.position="none"), 
             plot.box.Escore + theme(legend.position="none"), 
             plot.box.Oscore + theme(legend.position="none"),
             plot.box.Ascore + theme(legend.position="none"), 
             plot.box.Cscore + theme(legend.position="none"), 
             plot.box.Imp + theme(legend.position="none"), 
             plot.box.SS,
               layout_matrix = rbind(c(1, 1, 2, 2),
                                   c(3, 3, 4, 4),
                                   c(5, 5, 6, 6), 
                                   c(7, 7, NA, NA)),
               top = "Personality test score distribution",
               bottom = "",
               left = "")
```

```{r personality summary plot, fig.height=10, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(plot.density.Nscore + theme(legend.position="none"), 
             plot.density.Escore + theme(legend.position="none"), 
             plot.density.Oscore + theme(legend.position="none"),
             plot.density.Ascore + theme(legend.position="none"), 
             plot.density.Cscore + theme(legend.position="none"), 
             plot.density.Imp + theme(legend.position="none"), 
             plot.density.SS,
               layout_matrix = rbind(c(1, 1, 2, 2),
                                   c(3, 3, 4, 4),
                                   c(5, 5, 6, 6), 
                                   c(7, 7, NA, NA)),
             top = "Personality test score distribution",
             left = "Density")
```

For each test-score distribution, after checking for normality (Shapiro-Wilk test) we test the hypothesis that there is no difference between Users and Non-Users. We use the Student t-test when the data is normally distributed and the Mann-Whitney-Wilcoxon test otherwise.

* Neuroticism: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Nscore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Nscore.Used$p.value)`] < 0.05: the data are not normally distributed. 
Wilcox p-value = `r sprintf("%0.2f",wilcox.Nscore$p.value)` < 0.05: the 'User' and 'Non-User' population means are not identical.
* Extraversion: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Escore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Escore.Used$p.value)`] > 0.05: the data are normally distributed. 
t-test p-value = `r sprintf("%0.2f",t_test.Escore$p.value)` > 0.05: the 'User' and 'Non-User' population means are identical. For the variance, t-test p-value = `r sprintf("%0.2f",t_test.var.Escore$p.value)` > 0.05: the 'User' and 'Non-User' population variances are also identical. 
* Openness to experience: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Oscore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Oscore.Used$p.value)`] > 0.05: users are not normally distributed (but non-users are). 
Wilcox p-value = `r sprintf("%0.2f",wilcox.Oscore$p.value)` < 0.05: the 'User' and 'Non-User' population means are not identical.
* Agreeableness: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Ascore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Ascore.Used$p.value)`] > 0.05: the data are normally distributed. 
t-test p-value = `r sprintf("%0.2f",t_test.Ascore$p.value)` < 0.05: the 'User' and 'Non-User' population means are not identical.
* Conscientiousness: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Cscore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Cscore.Used$p.value)`] < 0.05:  the data are not normally distributed. Wilcox p-value = `r sprintf("%0.2f",wilcox.Cscore$p.value)` < 0.05: the 'User' and 'Non-User' population means are not identical.
* Impulsivity: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Impulsive.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Impulsive.Used$p.value)`] < 0.05:  the data are not normally distributed. Wilcox p-value = `r sprintf("%0.2f",wilcox.Impulsive$p.value)` < 0.05: the 'User' and 'Non-User' population means are not identical.
* Seeking sensations: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.SS.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.SS.Used$p.value)`] < 0.05:  the data are not normally distributed. Wilcox p-value = `r sprintf("%0.2f",wilcox.SS$p.value)` < 0.05: the 'User' and 'Non-User' population means are not identical.

In summary: 
```{r independence table, message=FALSE, paged.print=FALSE}
table.indep %>% knitr::kable()
```

While significant overlap is observed, most personality-related density plots show significant differences in the mean between users and non-users. particularly as it relates to openness to experience, agreeableness, conscientiousness, impulsivity, and seeking sensations. The observations are by and large consistent with intuition when it comes to openness to experience, impulsivity and seeking-sensations.

Some implications are rather entertaining, notably the notion that nice (Agreeable) people may be less likely to smoke cannabis, or conversely that exposure to pot might make people less nice. Likewise, either conscientious people tend to not use cannabis, or cannabis smoking tends to make people less meticulous.

On the other hand, the distribution means and variances for users and non-users as they relate to extraversion are statistically identical, suggesting that this personality trait may not impact cannabis consumption. The modeling section below examines whether this observation is consistent with the results derived from machine learning.

\newpage

## C: Modeling

```{r modeling plot parameters}
imp_text_size <-7
```

We seek a model which improves on the ratio of users to the population 
(`r sprintf("%0.1f%%", round((df.train %>% filter(Used == 1) %>% nrow()) / (nrow(df.train))*100, digits = 1))`). 
This naive approach constitutes the baseline above which predictive modeling is interesting.

### Pre-processing
#### Feature correlation
```{r correlation matrix utilities, message=FALSE, paged.print=FALSE}
# *** Utilities for correlation matrix plot
# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}

# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

# Reorder correlation matrix as a function of distance bw features
reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <- cormat[hc$order, hc$order]
}
# *** End utilities ***

corr_plot <- function(df, title) { # *** Main routine ***
  cormat <- round(cor(df, method = 'pearson'), 2)
  #cormat <- reorder_cormat(cormat)
  upper_tri <- get_upper_tri(cormat)
  upper_tri
  melted_cormat <- melt(upper_tri, na.rm = TRUE)
  # Create the plot
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +
    geom_tile(color = "white") +
    scale_fill_gradient2(low = "#998ec3", high = "#f1a340", mid = "#f7f7f7",
                         midpoint = 0., limit = c(-1,1), space = "Lab",
                         name="Pearson\nCorrelation") +
    theme_minimal()+ 
    theme(axis.text.x = element_text(angle = 45, vjust = 1,
                                     size = 12, hjust = 1))+
    theme(axis.text.y = element_text(vjust = 0,
                                     size = 12, hjust = 1))+
    coord_fixed() +
    ggtitle(title) +
    theme(
      plot.title = element_text(size = 16, face = 'bold'),
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      #panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.justification = c(1, 0),
      legend.position = c(0.55, 0.725),
      legend.direction = "horizontal") +
      guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                                    title.position = "top", title.hjust = 0.5))
  #Print heatmap
  print(ggheatmap)
  return(cormat)
}
```

We examine the most contributing cells to the total Chi-square score redundancies among the 12 predictors and with the Used class by examining the Pearson $\chi^2$ residuals.

```{r correlation plot, message=FALSE, paged.print=FALSE}
plot.corr
```

With a maximum correlation of 
`r max(abs(cormat-diag(dim(cormat)[1])))`
none of the features are strongly correlated among each other or with the used class.

Besides the goodness of fit to the test data, the demographic and personality-related observations above will guide the assessment of the models we derive.

#### Low-variance analysis
There are 0 variables that meet the low-variance removal threshold.

### Recursive Feature elimination
```{r rfe wrapper, message=FALSE, paged.print=FALSE}
# Wrapper for the RFE algorithm
rfe_drug <- function(df, outcomeName){ 
  # Remove the id column
  df <- df %>% select(-Id) 

  #Make the Used class a factor
  df$Used <- factor(df$Used,
                     levels = c(0, 1), 
                     labels = c("0", "1"))

  # RFE controls
  control <- rfeControl(functions = rfFuncs,
                        method = "repeatedcv",
                        repeats = 3, # Change to 10 for final run
                        verbose = FALSE)
  # Exclude Class from the list of predictors
  predictors <- names(df)[!names(df) %in% outcomeName]

  # caret RFE Call
  pred_Profile <- rfe(df[ ,predictors], 
                      unlist(df[ ,outcomeName]), 
                      rfeControl = control)
  return(pred_Profile)
}
```

For RFE as well as subsequent modeling, we use the k-fold cross validation method which involves splitting the dataset into k subsets. The algorithm holds aside one of the subsets while the model is trained on the others. This process is repeated a predetermined number of times and the overall accuracy estimate is provided.

```{r rfe call, message=FALSE, paged.print=FALSE}
# outcomeName <- "Used"
# set.seed(5)
# rfe_Profile <- rfe_drug(df.train, outcomeName)
# #rfe_Profile
# predictors <- predictors(rfe_Profile)
# imp <- varImp(rfe_Profile, scale = TRUE)
```

```{r rfe profile plot, message=FALSE, paged.print=FALSE}
plot.profile.rfe
```

After the RFE, we retain `r length(predictors)` features: 
`r predictors`

```{r rfe importance plot, message=FALSE, paged.print=FALSE}
plot.importance.rfe
```

The comparative analysis of the contribution of each feature agrees by and large with that of the density distribution: among the personality trait tests, the E-score contributes the least, as expected from the results of the t-test aboce. Seeking sensation and O-score contribute the most, as expected. 
We did not expect ethnicity to be much of a contributor and, although not eliminated by the RFE, this factor is by far the least significant.

#### Modeling

Our approach consists, for each of the six methods used, in starting off by training an unoptimized model which is in turn used as the starting point before determining a set of optimal tuning parameters by cross-validation. 

##### GLMnet

Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the lasso or elasticnet penalty at a grid of values for the regularization parameter lambda

Optimization of parameters for the glmnet model:

```{r glmnet plot, fig.height=5, fig.width=8, message=FALSE, paged.print=FALSE}
plot(model.glmnet, main = "GLMnet")
```

##### Decision trees

```{r rpart plot, fig.height=5, fig.width=8, message=FALSE, paged.print=FALSE}
plot(model.rpart, main = "rpart")
```

##### Random forest

```{r rf plot, fig.height=5, fig.width=8, message=FALSE, paged.print=FALSE}
plot(model.rf, main = "rf")
```

##### Stochastic gradient boosting

Optimal values of shrinkage and boosting iterations for the gbm model:

```{r gbm plot, fig.height=5, fig.width=8, message=FALSE, paged.print=FALSE}
plot(model.gbm, main = "gbm")
```

##### Neural network

Optimal values of decay and number of hidden units for the neural network:

```{r nnet plot, fig.height=5, fig.width=8, message=FALSE, paged.print=FALSE}
plot(model.nnet, main = "rf")
```

### Model comparisons
```{r variable importance plots, fig.height=10, fig.width=8, message=FALSE, paged.print=FALSE}
#Variable importance
grid.arrange(plot.varImp.glm, plot.varImp.glmnet,
             plot.varImp.rpart, plot.varImp.rf,
             plot.varImp.gbm, plot.varImp.nnet,
             nrow = 3,
             top = "Model feature importance",
             left = "Predictor",
             bottom = "Importance"
)
```

```{r model comparisons, message=FALSE, paged.print=FALSE}
plot.model.fit
```

\newpage

# Results

None of the modeling approaches used provided an improvement over generalized linear regression (`r sprintf("%0.1f%%",(CM.glm$overall)[1]*100)` accuracy). Besides offering the highest accuracy, the importance plot related to the glm model is by and large consistent with results from the data exploration:

+ Country of origin, Age, Openness to experiment and Education are the factors contributing most to the accuracy. 
+ Edu ation and Sensation-seeking have a significant importance
+ Ethnicity, N-scores and E-scores and  do not contribute much to the metric.
+ However the low impact of Impulsivity is at variance with the observations from data exploration. 

The confusion matrix for the GLM model has a sensitivity of (`r sprintf("%0.2f", CM.glm$byClass["Sensitivity"])`) and specificity of (`r sprintf("%0.2f", CM.glm$byClass["Specificity"])`):

```{r CM glm, message=FALSE, paged.print=FALSE}
CM.glm
```

The next best model (`r sprintf("%0.1f%%",(CM.nnet$overall)[1]*100)` accuracy) is obtained with a neural network. It suggests that demographic factors have more impact on cannabis use than personality (top three importance factors: country of origin, ethnicity, and age). Given the statistical similarity between the two ethnic groups discussed in the Personality analysis section, this model is somewhat unconvincing.

The confusion matrix for the neural network model is :
```{r CM nnet, message=FALSE, paged.print=FALSE}
CM.nnet
```

With 0 (Non-user) as the 'positive' class, the 3-point decrease in sensitivity (`r sprintf("%0.2f", CM.nnet$byClass["Sensitivity"])`) indicates a drop in this model's ability to predict Non-users. 

While less accurate, the random forest model (`r sprintf("%0.1f%%",(CM.rf$overall)[1]*100)` accuracy) gives a preponderant importance to country of origin, sensation-seeking trait, age and openness to experiment). It also gives no importance to ethnicity and E-score and less than expected to gender.

The confusion matrix for the random forest model is :
```{r CM rf, message=FALSE, paged.print=FALSE}
CM.rf
```

With RF also, the decrease in sensitivity (`r sprintf("%0.2f", CM.rf$byClass["Sensitivity"])`) indicates a drop in this model's ability to predict Non-users.

For this dataset, the optimized GLM offers the weakest modeling technique (`r sprintf("%0.1f%%",(CM.glmnet$overall)[1]*100)` accuracy), performing less well than the naive approach (`r sprintf("%0.1f%%", naive*100)` accuracy).

# Conclusion

With demographic factors being more predictive than personality, the modeling suggests that cannabis consumption is first and foremost a cultural phenomenon.

Generalized linear modeling offers the highest improvement (`r sprintf("%0.1f%%",((CM.glm$overall)[1]-naive)*100)`) over the naive approach and the neural network (`r sprintf("%0.1f%%",((CM.nnet$overall)[1]-naive)*100)`). While both values are similar,the GLM model agrees better with the results from the exploration and statistical analysis of the data.

In a previous run, we had labelled non-users only those that had never used cannabis and users all the others. In that case, machine learning offered only a modest improvement (81.1% for the neural network compared to 78% with the naive approach)
We felt this may be due to the choice of classification and that a potentially more insightful analysis would take into account frequency of use (this information was nevertheless not available in the dataset). 
