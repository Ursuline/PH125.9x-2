---
title: "Predicting Cannabis Consumption from Demographics and Personality"
subtitle: "HarvardX PH125.9x Data Science Capstone"
author: "Charles MÃ©gnin"
date: "10/8/2019"
output:
  pdf_document:
    toc: yes
  html_document:
    number_sections: no
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = TRUE,
	cache = FALSE,
	tidy = TRUE
)
```

```{r load R packages}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(mlbench)) install.packages("mlbench", repos = "http://cran.us.r-project.org")
if(!require(grid)) install.packages("grid", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(gplots)) install.packages("gplots", repos = "http://cran.us.r-project.org")
if(!require(graphics)) install.packages("graphics", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
load("drugEnvironment.RData")
```

# Executive Summary

## + Introduction

Drug use is a behavior that constitutes an important factor linked to poor health, including early mortality, and which presents significant adverse consequences for the social fabric, notably with respect to criminality and family cohesion. Early detection of an individual's predisposition to drug consumption offers healthcare professionals an opportunity to short-circuit the onset of addiction. 

The present study is based on a dataset that includes demographic and psychological information related to the consumption of 18 legal and illegal drugs by 1885 participants. For the purpose of this study, we choose to focus the data analysis and modeling on the use of cannabis.

## + Goal of project

The goal of this project is to assess whether an individual's consumption of cannabis can be predicted from a combination of demographic and personality data. 

To do so, we build and assess the effectiveness of six machine learning classifiers and confront the results obtained with the insights provided by data exploration.

## + Dataset description

The original dataset is found on the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29).
It is based the research paper by E. Fehrman, A. K. Muhammad, E. M. Mirkes, V. Egan and A. N. Gorban, ["The Five Factor Model of personality and evaluation of drug consumption risk.," arXiv, 2015](https://arxiv.org/abs/1506.06297). The data was collected from 1885 English-speaking participants over 18 years of age between March 2011 and March 2012.

The original dataset includes answers to questions related to the use of alcohol, amphetamines, amyl nitrite, benzodiazepines, cannabis, chocolate, cocaine, caffeine, crack, ecstasy, heroin, ketamine, legal highs, LSD, methadone, magic mushrooms, nicotine and volatile substance abuse (VSA)) and one fictitious drug (Semeron) which was introduced to identify over-claimers. In the present study, we restrict our scope to the analysis of cannabis consumption.

In the original dataset, drug use is separated between 'Never used', 'Used over a decade ago', 'Used in last decade', 'Used in last year', 'Used in last month', 'Used in last week' and 'Used in last day'. For the purpose of this study, we separate the data in two groups: 'Non-Users' which is a combination of 'Never used', 'Used over a decade ago', 'Used in last decade', and 'Used' (the combination of the others, concisting of 'Used in last year', 'Used in last month', 'Used in last week' and 'Used in last day'). We therefore create a classification that distinguishes those that have either never used cannabis or used it at least a decade ago and which we refer to as 'Non-users' from those that have used it more recently which we call 'Users'.

The data consists of two groups of pre-normalized and centered predictors: 

1. Five demographic predictors : Age, Gender, Level of education, Ethnicity, and Country of origin.

2. The results from seven scored tests administered to assess personality, specifically:

  + Neuroticism (a long-term tendency to experience negative emotions such as nervousness, tension, anxiety and depression);
  + Extraversion (manifested in outgoing, warm, active, assertive, talkative, cheerful, and in search of stimulation characteristics);
  + Openness to experience (a general appreciation for art, unusual ideas, and imaginative, creative, unconventional, and wide interests);
  + Agreeableness (a dimension of interpersonal relations, characterized by altruism, trust, modesty, kindness, compassion and cooperativeness);
  + Conscientiousness (a tendency to be organized and dependable, strong-willed, persistent, reliable, and efficient);
  + Impulsiveness;
  + Sensation-seeking.

The working dataset in this study therefore consists of one Class (Cannabis consumption labeled 'Used') and twelve predictors (five demographic and seven personality-related).

## + Key steps

We extract a training subset (80% of data) from the dataset for the purpose of training our model, and use the remaining 20% of the data as a test set for the purpose of evaluation. This being a classification problem, we use accuracy as the metric to assess the goodness of fit.

This report consists of two main sections:

+ In the first part, after performing minor data engineering, we explore, bin, and analyze the dataset. 
+ In the second part, we move on to the modeling phase:

  * After applying the Recursive Feature Elimination algorithm to seek and potentially discard predictors that do not contribute significantly to the outcome, we build models based on the following six popular machine learning methods:
  + Generalized linear model (glm)
  + Generalized linear model with penalized maximum likelihood (GLMnet)
  + Decision tree (rpart)
  + Random forest (rf)
  + Stochastic gradient boosting (gbm)
  + Neural network (nnet)

We compare the modeling approaches, both in terms of accuracy and coherence with the data analysis.

\newpage 

# Analysis

## A: Data engineering
* All predictors were already normalized and centered in the original dataset.
* We construct the 'Used' class to separate 'Never used' participants (0) from the others (1).
* We then partition the data between training (80% - `r nrow(df.train)` participants) and test sets (20% - `r nrow(df.test)` participants), preserving the distribution of the Cannabis class.
* There are `r sum(anyNA(df.raw))` NAs in the dataset as a whole (no interpolation to perform)

## B: Data exploration
```{r plot parameters, message=FALSE, warning=FALSE, paged.print=FALSE}
# Global plot parameters
fill <- 'skyblue3'
color <- 'grey'
fill_no <- "#af8dc3"
fill_yes <- "#7fbf7b"
used_colors <- c(fill_no, fill_yes) # No/Yes
alpha <- 0.4 # alpha-blending
axis_text_size <- 10
```


#### Class distribution
```{r overall consumption, message=FALSE, warning=FALSE, paged.print=FALSE}
plot.use
```

The training set of `r df.train %>% nrow()` participants consists of 
`r df.train %>% filter(Used == 1) %>% nrow()` 
participants (`r sprintf("%0.1f%%", round(df.train %>% filter(Used == 1) %>% nrow()/nrow(df.train)*100, digits = 1))`) having used cannabis and 
`r df.train %>% filter(Used == 0) %>% nrow()` who never have (`r sprintf("%0.1f%%", round(df.train %>% filter(Used == 0) %>% nrow()/nrow(df.train)*100, digits = 1))`), for a user-to-non-user ratio of 1:`r sprintf("%0.1f", round(df.train %>% filter(Used == 1) %>% nrow()/df.train %>% filter(Used == 0) %>% nrow(), digits = 1))`

### Contingency plots (prior to binning)

#### Cannabis use by demographic group
```{r contingency plots before binning, fig.height=5, fig.width=8, message=FALSE, paged.print=FALSE}
# Age contingency plot 
plot.balloon.age
# Gender contingency plot 
plot.balloon.gender
# Education contingency plot 
plot.balloon.edu
# Country contingency plot 
plot.balloon.country
# Ethnicity contingency plot 
plot.balloon.ethn
```

The dataset is dominated by young and educated white American and British participants of both sexes.

#### Binning
The small size of many demographic sub-groups add little valuable insight and will likely only serve to introduce variance in the analysis. 
At the risk of erasing behavioral differences among groups, the distribution of the dataset forces a more meaningful binning of the demographic information. In particular and in addition to the small individual population sizes, we can think of no valid reason to distinguish among those that left school before 16, at 16, at 17 or at 18 and lump these in a "Left school as a teen" group.

- 5 age groups: "18-24", "25-34", "35-44", "45-54", "55+"
- 5 groups for Education: "Left school as a teen", "Some college", "Professional certificate", "University degree", "Masters degree" and "Doctorate".
- 3 groups for Country: "USA", "UK", "Others".
- 2 ethnic groups: "Whites", "Non-whites".

```{r rebin demographics, message=FALSE, paged.print=FALSE}
df.train <- 
  df.train %>% 
  mutate(Country = ifelse(Country %in% c(-0.09765, 0.24923, -0.46841, 0.21128), -0.28519, Country)) %>%
  mutate(Age = ifelse(Age == 2.59171, 1.82213, Age)) %>%
  mutate(Education = ifelse(Education %in% c(-2.43591, -1.73790, -1.43719), -1.22751, # Dropped school
                            ifelse(Education == 1.98437, 1.16365, Education))) %>% # Merge MS & PhD 
  mutate(Ethnicity = ifelse(Ethnicity != -0.31685, 0.11440, Ethnicity))

df.test <- 
  df.test %>% 
  mutate(Country = ifelse(Country %in% c(-0.09765, 0.24923, -0.46841, 0.21128), -0.28519, Country)) %>%
  mutate(Age = ifelse(Age == 2.59171, 1.82213, Age)) %>%
  mutate(Education = ifelse(Education %in% c(-2.43591, -1.73790, -1.43719), -1.22751, # Dropped school
                            ifelse(Education == 1.98437, 1.16365, Education))) %>% # Merge MS & PhD 
  mutate(Ethnicity = ifelse(Ethnicity != -0.31685, 0.11440, Ethnicity))
```

### Analysis of demographics
```{r contingency plot, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(plot.country, plot.gender, plot.ethn,
             plot.age, plot.edu,
             layout_matrix = rbind(c(1, 1, 2, 2, 3, 3), 
                                   c(4, 4, 4, 5, 5, 5)),
             top = "Use of cannabis in training set by:",
             left = "Counts")
```

Users outnumber non-users in many demographic sub-group, particularly for men, in the USA, among 18-24 year olds and for those that didn't complete a college degree. In the modeling phase, we therefore expect the corresponding predictors (country, age group, education and gender) to have significant weight.

```{r ratio plot, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(plot.prop.country, plot.prop.gender, plot.prop.ethn,
             plot.prop.age, plot.prop.edu,
             layout_matrix = rbind(c(1, 1, 2, 2, 3, 3), 
                                   c(4, 4, 4, 5, 5, 5)),
             top = "Ratio of Users to Non-Users by demographic group",
             left = "ratio(Users : Non-Users)")
```

For better readability of the differences among them, we use the ratio of users to non-users as index of consumption for each demographic sub-group.

* Americans consume more cannabis (`r sprintf("%0.1f", round((table.country %>% filter(Var == "USA"))$Prop, digits = 1))`) 
than all other nationalities, and particularly the British 
(`r sprintf("%0.1f", round((table.country %>% filter(Var == "UK"))$Prop, digits = 1))`) 
who, along with females 
(`r sprintf("%0.1f", round((table.gender %>% filter(Var == "female"))$Prop, digits = 1))`), 
people over 34 years of age and people with degrees, are among the lowest using group with more non-consumers than consumers.
* Men consume more 
(`r sprintf("%0.1f", round((table.gender %>% filter(Var == "male"))$Prop, digits = 1))`)
than women
(`r sprintf("%0.1f", round((table.gender %>% filter(Var == "female"))$Prop, digits = 1))`)
* Whites consume about the same as other ethnic groups 
(`r sprintf("%0.1f", round((table.ethn %>% filter(Var == "White"))$Prop, digits = 1))`)
and we do not expect ethnicity to be a significant factor.
(`r sprintf("%0.1f", round((table.ethn %>% filter(Var == "Non-white"))$Prop, digits = 1))`)
* Those having only had some college/university are the highest users in the education sub-group
(`r sprintf("%0.1f", round((table.edu %>% filter(Var == "Some college/univ."))$Prop, digits = 1))`)
* Finally, we observe a steady decline of cannabis use with age from 18-24 year olds
(`r sprintf("%0.1f", round((table.age %>% filter(Var == "18-24"))$Prop, digits = 1))`)
to participants over 55
(`r sprintf("%0.1f", round((table.age %>% filter(Var == "55+"))$Prop, digits = 1))`)
This points to a generational phenomenon (for instance, roughly five times as many 25-34 year-olds abstain when compared to 18-24 years olds, even though they have had ten additional years to experiment). 

\newpage
### Personality analysis
```{r personality plot parameters, message=FALSE, warning=FALSE, paged.print=FALSE}
breaks <- seq(-3, 3, .5)
angle <- 60
```
```{r neuroticism, message=FALSE, paged.print=FALSE}
# Neuroticism (N-score) plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Nscore))

NscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Nscore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Neuroticism",
       x = "N-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5))  +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2])+
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r extraversion, message=FALSE, paged.print=FALSE}
# Extraversion (E-score) plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Escore))

EscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Escore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Extraversion",
       x = "E-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5))  +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2])+
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r openness to experience, message=FALSE, paged.print=FALSE}
# Openness to experience (O-score) plot 
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Oscore))

OscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Oscore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Openness to experience",
       x = "O-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5))  +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r agreeableness, message=FALSE, paged.print=FALSE}
# Agreeableness (A-score) plot]
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Ascore))

AscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Ascore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Agreeableness",
       x = "A-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5))  +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r conscientiousness, message=FALSE, paged.print=FALSE}
# Conscientiousness (C-score) plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Cscore))

CscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Cscore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Conscientiousness",
       x = "C-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5)) +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))

```
```{r impulsiveness, message=FALSE, paged.print=FALSE}
# Impulsiveness plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Impulsive))

ImpDensityPlot <- df.train %>% 
  ggplot(aes(x = Impulsive, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Impulsivity",
       x = "Impulsivity score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5)) +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r SS, message=FALSE, paged.print=FALSE}
# Sensation-seeking plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(SS))

SSDensityPlot <- df.train %>% 
  ggplot(aes(x = SS, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Seeking sensations",
                     x = "Sensation-seeking score",
                     y = "") +
  scale_fill_manual(name = "Used?", 
                     values = c("0" = used_colors[1], "1" = used_colors[2]), 
                     labels = c("No", "Yes")) +
  scale_y_continuous(limits = c(0, .5)) +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```

```{r personality summary plot, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(plot.density.Nscore, plot.density.Escore, plot.density.Oscore,
             plot.density.Ascore, plot.density.Cscore, plot.density.Imp, plot.density.SS,
             layout_matrix = rbind(c(1, 1, 2, 2, 3, 3), 
                                   c(4, 4, 5, 5, 6, 6), 
                                   c(7, 7, 7, NA, NA, NA)),
             top = "Personality test score distribution",
             left = "Density")
```

For each test-score distribution, after checking for normality (Shapiro-Wilk test) we test the hypothesis that there is no difference between Users and Non-Users. We use the Student t-test if the data is normally distributed and the Mann-Whitney-Wilcoxon test otherwise.

* Neuroticism: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Nscore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Nscore.Used$p.value)`] < 0.05: the data are not normally distributed. 
Wilcox p-value = `r sprintf("%0.2f",wilcox.Nscore$p.value)` < 0.05: the 'User' and 'Non-User' populations are not identical.
* Extraversion: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Escore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Escore.Used$p.value)`] > 0.05: the data are normally distributed. 
t-test p-value = `r sprintf("%0.2f",t_test.Escore$p.value)` > 0.05: the 'User' and 'Non-User' populations are identical.
* Openness to experience: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Oscore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Oscore.Used$p.value)`] > 0.05: users are not normally distributed (but non-users are). 
Wilcox p-value = `r sprintf("%0.2f",wilcox.Oscore$p.value)` < 0.05: the 'User' and 'Non-User' populations are not identical.
* Agreeableness: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Ascore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Ascore.Used$p.value)`] > 0.05: the data are normally distributed. 
t-test p-value = `r sprintf("%0.2f",t_test.Ascore$p.value)` < 0.05: the 'User' and 'Non-User' populations are not identical.
* Conscientiousness: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Cscore.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Cscore.Used$p.value)`] < 0.05:  the data are not normally distributed. Wilcox p-value = `r sprintf("%0.2f",wilcox.Cscore$p.value)` < 0.05: the 'User' and 'Non-User' populations are not identical.
* Impulsivity: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.Impulsive.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.Impulsive.Used$p.value)`] < 0.05:  the data are not normally distributed. Wilcox p-value = `r sprintf("%0.2f",wilcox.Impulsive$p.value)` < 0.05: the 'User' and 'Non-User' populations are not identical.
* Seeking sensations: Shapiro p-values = [Not Used: `r sprintf("%0.2f",shapiro.SS.notUsed$p.value)`, Used: `r sprintf("%0.2f",shapiro.SS.Used$p.value)`] < 0.05:  the data are not normally distributed. Wilcox p-value = `r sprintf("%0.2f",wilcox.SS$p.value)` < 0.05: the 'User' and 'Non-User' populations are not identical.

In summary: 
```{r independence table, message=FALSE, paged.print=FALSE}
table.indep %>% knitr::kable()
```

While significant overlap is observed, most personality-related density plots show differences between users and non-users. particularly as it relates to openness to experience, agreeableness, conscientiousness, impulsivity, and seeking sensations. The observations are by and large consistent with intuition when it comes to openness to experience, impulsivity and seeking-sensations.

Some implications are rather entertaining, notably the notion that nice (Agreeable) people may be less likely to smoke cannabis, or conversely that exposure to pot might make people less nice. Likewise, either conscientious people tend to not use cannabis, or cannabis smoking tends to make people less meticulous.

On the other hand, the distributions for users and non-users as they relate to extraversion are statistically identical, suggesting that this personality trait may not impact cannabis consumption. We will examine in the modeling section below whether this observation is consistent with the results derived from machine learning.

\newpage

## C: Modeling

```{r modeling plot parameters}
imp_text_size <-7
```

We seek a model which improves on the ratio of users to the population 
(`r sprintf("%0.2f%%", round((df.train %>% filter(Used == 1) %>% nrow()) / (nrow(df.train))*100, digits = 1))`). 
This constitutes the baseline above which predictive modeling is interesting.

### Pre-processing
#### Feature correlation
```{r correlation matrix utilities, message=FALSE, paged.print=FALSE}
# *** Utilities for correlation matrix plot
# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}

# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

# Reorder correlation matrix as a function of distance bw features
reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <- cormat[hc$order, hc$order]
}
# *** End utilities ***

corr_plot <- function(df, title) { # *** Main routine ***
  cormat <- round(cor(df, method = 'pearson'), 2)
  #cormat <- reorder_cormat(cormat)
  upper_tri <- get_upper_tri(cormat)
  upper_tri
  melted_cormat <- melt(upper_tri, na.rm = TRUE)
  # Create the plot
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +
    geom_tile(color = "white") +
    scale_fill_gradient2(low = "#998ec3", high = "#f1a340", mid = "#f7f7f7",
                         midpoint = 0., limit = c(-1,1), space = "Lab",
                         name="Pearson\nCorrelation") +
    theme_minimal()+ 
    theme(axis.text.x = element_text(angle = 45, vjust = 1,
                                     size = 12, hjust = 1))+
    theme(axis.text.y = element_text(vjust = 0,
                                     size = 12, hjust = 1))+
    coord_fixed() +
    ggtitle(title) +
    theme(
      plot.title = element_text(size = 16, face = 'bold'),
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      #panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.justification = c(1, 0),
      legend.position = c(0.55, 0.725),
      legend.direction = "horizontal") +
      guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                                    title.position = "top", title.hjust = 0.5))
  #Print heatmap
  print(ggheatmap)
  return(cormat)
}
```

We examine the most contributing cells to the total Chi-square score redundancies among the 12 predictors and with the Used class by examining the Pearson $\chi^2$ residuals.

```{r correlation plot, message=FALSE, paged.print=FALSE}
plot.corr
```

With a maximum correlation of 
`r max(abs(cormat-diag(dim(cormat)[1])))`
none of the features are strongly correlated among each other or with the used class.

Besides the goodness of fit to the test data, the demographic and personality-related observations above will guide the assessment of the models we derive.

#### Low-variance analysis
There are 0 variables that meet the low-variance removal threshold.

### Recursive Feature elimination
```{r rfe wrapper, message=FALSE, paged.print=FALSE}
# Wrapper for the RFE algorithm
rfe_drug <- function(df, outcomeName){ 
  # Remove the id column
  df <- df %>% select(-Id) 

  #Make the Used class a factor
  df$Used <- factor(df$Used,
                     levels = c(0, 1), 
                     labels = c("0", "1"))

  # RFE controls
  control <- rfeControl(functions = rfFuncs,
                        method = "repeatedcv",
                        repeats = 3, # Change to 10 for final run
                        verbose = FALSE)
  # Exclude Class from the list of predictors
  predictors <- names(df)[!names(df) %in% outcomeName]

  # caret RFE Call
  pred_Profile <- rfe(df[ ,predictors], 
                      unlist(df[ ,outcomeName]), 
                      rfeControl = control)
  return(pred_Profile)
}
```

For RFE as well as subsequent modeling, we use the k-fold cross validation method which involves splitting the dataset into k subsets. The algorithm holds aside one of the subsets while the model is trained on the others. This process is repeated a predetermined number of times and the overall accuracy estimate is provided.

```{r rfe call, message=FALSE, paged.print=FALSE}
# outcomeName <- "Used"
# set.seed(5)
# rfe_Profile <- rfe_drug(df.train, outcomeName)
# #rfe_Profile
# predictors <- predictors(rfe_Profile)
# imp <- varImp(rfe_Profile, scale = TRUE)
```

```{r rfe profile plot, message=FALSE, paged.print=FALSE}
plot.profile.rfe
```

After the RFE, we retain `r length(predictors)` predictors: 
`r predictors`

```{r rfe importance plot, message=FALSE, paged.print=FALSE}
plot.importance.rfe
```

The comparative analysis of the contribution of each factor agrees by and large with that of the density distribution: among the personality trait tests, the E-score contributes the least, as expected from the results of the t-test aboce. Seeking sensation and O-score contribute the most, as expected. 
We did not expect ethnicity to be much of a contributor and, although not eliminated by the RFE, this factor is by far the least significant.

#### Modeling

Our approach consists, for each of the six methods used, in starting off by training an unoptimized model which is in turn used as the starting point before determining an set of optimal tuning parameters by cross-validation. The example below is the method used to determine the number of boosting iterations and shrinkage for the Stochastic gradient boosting algorithm. A similar method is used for the neural network and the random forest.

```{r gbm plot, message=FALSE, paged.print=FALSE}
plot.level.gbm
```

### Model comparisons
```{r variable importance plots, fig.height=10, fig.width=8, message=FALSE, paged.print=FALSE}
#Variable importance
grid.arrange(plot.varImp.glm, plot.varImp.glmnet,
             plot.varImp.rpart, plot.varImp.rf,
             plot.varImp.gbm, plot.varImp.nnet,
             nrow = 3,
             top = "Model predictor importance",
             left = "Predictor",
             bottom = "Importance"
)
```

```{r model comparisons, message=FALSE, paged.print=FALSE}
plot.model.fit
```


# Results

The best model (`r sprintf("%0.1f%%",(CM.nnet$overall)[1]*100)` accuracy) is obtained by training a neural network. It suggests that demographic factors have more impact on cannabis use than personality (top three importance factors: ethnicity, gender and country of origin).
Besides offering the highest accuracy, the importance plot related to the model is largely consistent with results from the data exploration, notably personality factors whose density distributions do not allow to distinguish users from non-users, also have a low importance score:

+ N-scores and E-scores do not contribute much to the metric.
+ C-score, O-score, and Sensation-seeking are impactful.
+ However the low impact of Impulsivity is at variance with the observations from data exploration.
+ The model attributes to ethnicity an importance we didn't expect from the exploration of the data.

It is interesting that the results from random forest modeling, while slightly less successful than those from the neural network, present a very different picture, with personality being the overwhelming set of factors contributing to cannabis comsumption (top six importance factors). 

For this dataset, the optimized GLM offers the weakest modeling technique, performing less well than the naive approach.

# Conclusion

With demographic factors being more predictive than personality, the modeling suggests that cannabis consumption is primarily a cultural phenomenon.

Leaving aside linear modeling, the neural network offers an improvement of (`r sprintf("%0.1f%%",((CM.nnet$overall)[1]-naive)*100)`) over the naive approach.

In a previous run, we had labelled non-users only those that had never used cannabis and users all the others. In that case, machine learning offered only a modest improvement (81.1% for the neural network compared to 78% with the naive approach)
We felt this may be due to the choice of classification and that a potentially more insightful approach would take into account frequency of use (this information was nevertheless not available in the dataset). It might also be interesting to bin together Never users with those not having used over a long period of time.
