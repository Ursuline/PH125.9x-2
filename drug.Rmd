---
title: "Predicting Cannabis Consumption from Demographics and Personality"
subtitle: "HarvardX PH125.9x Data Science Capstone"
author: "Charles MÃ©gnin"
date: "10/8/2019"
output:
  html_document:
    number_sections: no
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = TRUE,
	cache = FALSE,
	tidy = TRUE
)
```

```{r load R packages}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(mlbench)) install.packages("mlbench", repos = "http://cran.us.r-project.org")
if(!require(grid)) install.packages("grid", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(gplots)) install.packages("gplots", repos = "http://cran.us.r-project.org")
if(!require(graphics)) install.packages("graphics", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
load("drugEnvironment.RData")
```

```{r load data, message=TRUE, warning=TRUE}
# Remote / local flag
remote <- 0

if(remote == 1) { # Load data file from github repository
  df.raw <- read.table("https://raw.githubusercontent.com/Ursuline/PH125.9x-2/master/data/drug_consumption.csv", 
                      header = FALSE,
                      sep = ",")
} else { # Load local copy
df.raw <- read.csv("./data/drug_consumption.csv", header = FALSE)
}
# Add a header row
names(df.raw) <- c("Id", "Age", "Gender", "Education", "Country", "Ethnicity",
                   "Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS",
                   "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis", "Choc", 
                   "Coke", "Crack", "Ecstasy", "Heroin", "Ketamine","Legalh", "LSD", 
                   "Meth", "Mushrooms", "Nicotine", "Semer", "VSA")
```

# Executive Summary

## + Introduction

Drug use is a behavior that constitutes an important factor linked to poor health, including early mortality, and which presents significant adverse consequences for the social fabric, notably with respect to criminality and family cohesion. Early detection of an individual's predisposition to drug consumption offers healthcare professionals an opportunity to short-circuit the onset of addiction. 

The present study is based on a dataset that includes demographic and psychological information related to the consumption of 18 legal and illegal drugs by 1885 participants. For the purpose of this study, we choose to focus the data analysis and modeling on the use of cannabis.

## + Goal of project

The goal of this project is to assess whether an individual's consumption of cannabis can be predicted from a combination of demographic and personality data. 

To do so, we build and assess the effectiveness of six machine learning classifiers and confront the results obtained with the insights provided by data exploration.

## + Dataset description

The original dataset is found on the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29).
It is based the research paper by E. Fehrman, A. K. Muhammad, E. M. Mirkes, V. Egan and A. N. Gorban, ["The Five Factor Model of personality and evaluation of drug consumption risk.," arXiv, 2015](https://arxiv.org/abs/1506.06297). The data was collected from 1885 English-speaking participants over 18 years of age between March 2011 and March 2012.

In the original dataset, drug use is separated between 'Never used', 'Used over a decade ago', 'Used over a decade ago', 'Used in last decade', 'Used in last year', 'Used in last month', 'Used in last week' and 'Used in last day'. For the purpose of this study, we separate the data in two groups: 'Never Used' (the original predictor) and 'Used' (the combination of the others, representing people having used cannabis at least once in their lifetime).

The original dataset includes answers to questions related to the use of alcohol, amphetamines, amyl nitrite, benzodiazepines, cannabis, chocolate, cocaine, caffeine, crack, ecstasy, heroin, ketamine, legal highs, LSD, methadone, magic mushrooms, nicotine and volatile substance abuse (VSA)) and one fictitious drug (Semeron) which was introduced to identify over-claimers. In the present study, we restrict our scope to the analysis of cannabis consumption.

The data consists of two groups of pre-normalized and centered predictors: 

1. Five demographic predictors : Age, Gender, Level of education, Ethnicity, and Country of origin.

2. The results from seven scored tests administered to assess personality, specifically:

  + Neuroticism (a long-term tendency to experience negative emotions such as nervousness, tension, anxiety and depression);
  + Extraversion (manifested in outgoing, warm, active, assertive, talkative, cheerful, and in search of stimulation characteristics);
  + Openness to experience (a general appreciation for art, unusual ideas, and imaginative, creative, unconventional, and wide interests);
  + Agreeableness (a dimension of interpersonal relations, characterized by altruism, trust, modesty, kindness, compassion and cooperativeness);
  + Conscientiousness (a tendency to be organized and dependable, strong-willed, persistent, reliable, and efficient);
  + Impulsiveness;
  + Sensation-seeking.

The working dataset in this study therefore consists of one Class (Cannabis consumption labeled 'Used') and twelve predictors (five demographic and seven personality-related).

## + Key steps

We extract a training subset (80% of data) from the dataset for the purpose of training our model, and use the remaining 20% of the data as a test set for the purpose of evaluation. This being a classification problem, we use accuracy as the metric to assess the goodness of fit.

This report consists of two main sections:

+ In the first part, after performing minor data engineering, we explore, bin, and analyze the dataset. 
+ In the second part, we move on to the modeling phase:

  * After applying a Recursive feature elimination algotirhm to seek and discard predictors that do not contribute significantly to the outcome, we build models based on the following methods:
  + Generalized linear model (glm)
  + Generalized linear model with penalized maximum likelihood (GLMnet)
  + Decision tree (rpart)
  + Random forest (rf)
  + Stochastic gradient boosting (gbm)
  + Neural network (nnet)

We compare the modeling approaches, both in terms of accuracy and coherence with the data analysis.

\newpage 

# Analysis

## A: Data engineering
* All predictors were already normalized and centered in the original dataset.

* We construct the 'Used' class to separate 'Never used' participants (0) from the others (1).

```{r add Used class, message=FALSE, paged.print=FALSE}
# Create a 'Used' column to separate CL0 (never used) from the others 
df.raw <- df.raw %>% mutate(Used = ifelse(Cannabis == "CL0", 0, 1))

# Change the Used predictor to a factor
df.raw[,'Used'] <- factor(as.character(df.raw[,'Used']))

# Keep Used as the only Class. 
df.raw <- df.raw %>% 
  select("Id", "Age", "Gender", "Education", "Country", "Ethnicity", "Nscore", 
         "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS", 
         "Used")
```

* We then partition the data between training (80% / df.train) and test sets (20% / df.test) preserving the distribution of the Cannabis class.

```{r training / testing partition, message=FALSE, paged.print=FALSE}
set.seed(15)
trainIndex <- createDataPartition(df.raw$Used, 
                                  p = .8, 
                                  list = FALSE, 
                                  times = 1)
df.train <- df.raw[ trainIndex,]
df.test  <- df.raw[-trainIndex,]
```

\newpage
## B: Data exploration
```{r plot parameters, message=FALSE, warning=FALSE, paged.print=FALSE}
# Global plot parameters
fill <- 'skyblue3'
color <- 'grey'
fill_no <- "#af8dc3"
fill_yes <- "#7fbf7b"
used_colors <- c(fill_no, fill_yes) # No/Yes
alpha <- 0.4 # alpha-blending
axis_text_size <- 10
```

* There are `r sum(anyNA(df.raw))` NAs in the dataset.

#### Class distribution
```{r overall consumption, message=FALSE, warning=FALSE, paged.print=FALSE}
plot.use
```

The training set consists of 
`r df.train %>% filter(Used == 1) %>% nrow()` 
participants having used cannabis at some pioint in their lives and 
`r df.train %>% filter(Used == 0) %>% nrow()` 
participants who haven't for a user-to-non-user ratio of 1:
`r sprintf("%0.1f", round(df.train %>% filter(Used == 1) %>% nrow()/df.train %>% filter(Used == 0) %>% nrow(), digits = 1))`.

### Contingency plots (prior to binning)

#### Cannabis use by demographic group
```{r contingency plots before binning, fig.height=5, fig.width=8, message=FALSE, paged.print=FALSE}
# Age contingency plot 
plot.balloon.age
# Gender contingency plot 
plot.balloon.gender
# Education contingency plot 
plot.balloon.edu
# Country contingency plot 
plot.balloon.country
# Ethnicity contingency plot 
plot.balloon.ethn
```

We note that the dataset of `r df.train %>% nrow()` participants is dominated by young and educated white American and British participants of both sexes.

#### Binning
The small size of many demographic sub-groups add little valuable insight and will likely only serve to introduce variance in the analysis. At the risk of erasing behavioral differences among groups, the distribution of the dataset forces a more meaningful binning of the demographic information:

- 5 age groups: "18-24", "25-34", "35-44", "45-54", "55+"
- 5 groups for Education: "Left school as a teen", "Some college", "Professional certificate", "University degree", "Graduate degree".
- 3 groups for Country: "USA", "UK", "Others".
- 2 ethnic groups: "Whites", "Non-whites"

```{r rebin demographics, message=FALSE, paged.print=FALSE}
df.train <- 
  df.train %>% 
  mutate(Country = ifelse(Country %in% c(-0.09765, 0.24923, -0.46841, 0.21128), -0.28519, Country)) %>%
  mutate(Age = ifelse(Age == 2.59171, 1.82213, Age)) %>%
  mutate(Education = ifelse(Education %in% c(-2.43591, -1.73790, -1.43719), -1.22751, # Dropped school
                            ifelse(Education == 1.98437, 1.16365, Education))) %>% # Merge MS & PhD 
  mutate(Ethnicity = ifelse(Ethnicity != -0.31685, 0.11440, Ethnicity))

df.test <- 
  df.test %>% 
  mutate(Country = ifelse(Country %in% c(-0.09765, 0.24923, -0.46841, 0.21128), -0.28519, Country)) %>%
  mutate(Age = ifelse(Age == 2.59171, 1.82213, Age)) %>%
  mutate(Education = ifelse(Education %in% c(-2.43591, -1.73790, -1.43719), -1.22751, # Dropped school
                            ifelse(Education == 1.98437, 1.16365, Education))) %>% # Merge MS & PhD 
  mutate(Ethnicity = ifelse(Ethnicity != -0.31685, 0.11440, Ethnicity))
```

### Analysis of demographics
```{r contingency plot, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(plot.country, plot.gender, plot.ethn,
             plot.age, plot.edu,
             layout_matrix = rbind(c(1, 1, 2, 2, 3, 3), 
                                   c(4, 4, 4, 5, 5, 5)),
             top = "Use of cannabis in training set by:",
             left = "Counts")
```

\newpage
### Personality analysis
```{r personality plot parameters, message=FALSE, warning=FALSE, paged.print=FALSE}
breaks <- seq(-3, 3, .5)
angle <- 60
```
```{r neuroticism, message=FALSE, paged.print=FALSE}
# Neuroticism (N-score) plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Nscore))

NscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Nscore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Neuroticism",
       x = "N-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5))  +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2])+
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r extraversion, message=FALSE, paged.print=FALSE}
# Extraversion (E-score) plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Escore))

EscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Escore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Extraversion",
       x = "E-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5))  +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2])+
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r openness to experience, message=FALSE, paged.print=FALSE}
# Openness to experience (O-score) plot 
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Oscore))

OscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Oscore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Openness to experience",
       x = "O-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5))  +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r agreeableness, message=FALSE, paged.print=FALSE}
# Agreeableness (A-score) plot]
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Ascore))

AscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Ascore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Agreeableness",
       x = "A-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5))  +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r conscientiousness, message=FALSE, paged.print=FALSE}
# Conscientiousness (C-score) plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Cscore))

CscoreDensityPlot <- df.train %>% 
  ggplot(aes(x = Cscore, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Conscientiousness",
       x = "C-score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5)) +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))

```
```{r impulsiveness, message=FALSE, paged.print=FALSE}
# Impulsiveness plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(Impulsive))

ImpDensityPlot <- df.train %>% 
  ggplot(aes(x = Impulsive, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Impulsivity",
       x = "Impulsivity score",
       y = "") +
  scale_fill_manual(name = "Used?", 
                    values = c("0" = used_colors[1], 
                               "1" = used_colors[2]), 
                    labels = c("No", "Yes"),
                    guide = FALSE) +
  scale_y_continuous(limits = c(0, .5)) +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```
```{r SS, message=FALSE, paged.print=FALSE}
# Sensation-seeking plot
mean.score <- df.train %>% 
  group_by(Used) %>% 
  dplyr::summarize(count = n(), mean = mean(SS))

SSDensityPlot <- df.train %>% 
  ggplot(aes(x = SS, fill = Used, color = I(color))) +
  geom_density(alpha = alpha) +
  labs(title = "Seeking sensations",
                     x = "Sensation-seeking score",
                     y = "") +
  scale_fill_manual(name = "Used?", 
                     values = c("0" = used_colors[1], "1" = used_colors[2]), 
                     labels = c("No", "Yes")) +
  scale_y_continuous(limits = c(0, .5)) +
  scale_x_continuous(limits = c(-3, 3), breaks = breaks) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[1,3]), color = used_colors[1]) +
  geom_vline(linetype="dashed", xintercept  = as.numeric(mean.score[2,3]), color = used_colors[2]) +
  theme(axis.text.x = element_text(angle = angle, hjust = 1))
```

```{r personality summary plot, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(plot.density.Nscore, plot.density.Escore, plot.density.Oscore,
             plot.density.Ascore, plot.density.Cscore, plot.density.Imp, plot.density.SS,
             layout_matrix = rbind(c(1, 1, 2, 2, 3, 3), 
                                   c(4, 4, 5, 5, 6, 6), 
                                   c(7, 7, 7, NA, NA, NA)),
             top = "Personality test score distribution",
             left = "Density")
```

The density plots show some measure of difference between users and non-users particularly as it relates to either openness to experience, agreeableness, conscientiousness, impulsivity, and sensation-seeking. 
Some implications are rather entertaining, notably the notion that nice (ie: agreeable) people may be less likely to smoke cannabis, or conversely that exposure to pot might make people less nice.

On the other hand, the distributions for users and non-users as they relate to  neuroticism or extraversion are similar, suggesting that these personality traits may not impact cannabis consumption. We will examine in the modeling section below whether that is indeed the case.

### Feature correlation
```{r correlation matrix utilities, message=FALSE, paged.print=FALSE}
# *** Utilities for correlation matrix plot
# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}

# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

# Reorder correlation matrix as a function of distance bw features
reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <- cormat[hc$order, hc$order]
}
# *** End utilities ***

corr_plot <- function(df, title) { # *** Main routine ***
  cormat <- round(cor(df, method = 'pearson'), 2)
  #cormat <- reorder_cormat(cormat)
  upper_tri <- get_upper_tri(cormat)
  upper_tri
  melted_cormat <- melt(upper_tri, na.rm = TRUE)
  # Create the plot
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +
    geom_tile(color = "white") +
    scale_fill_gradient2(low = "#998ec3", high = "#f1a340", mid = "#f7f7f7",
                         midpoint = 0., limit = c(-1,1), space = "Lab",
                         name="Pearson\nCorrelation") +
    theme_minimal()+ 
    theme(axis.text.x = element_text(angle = 45, vjust = 1,
                                     size = 12, hjust = 1))+
    theme(axis.text.y = element_text(vjust = 0,
                                     size = 12, hjust = 1))+
    coord_fixed() +
    ggtitle(title) +
    theme(
      plot.title = element_text(size = 16, face = 'bold'),
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      #panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.justification = c(1, 0),
      legend.position = c(0.55, 0.725),
      legend.direction = "horizontal") +
      guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                                    title.position = "top", title.hjust = 0.5))
  #Print heatmap
  print(ggheatmap)
  return(cormat)
}
```

We examine redundancies among the 12 predictors and with the Used class:

```{r correlation plot, message=FALSE, paged.print=FALSE}
plot.corr
```

Besides the goodness of fit to the test data, the demographic and personality-related observations above will guide the assessment of the models we derive. 

\newpage

## C: Modeling

```{r modeling plot parameters}
imp_text_size <-7
```

We seek a model which improves on the ratio of users to the population 
(`r sprintf("%0.2f%%", round((df.train %>% filter(Used == 1) %>% nrow()) / (nrow(df.train))*100, digits = 1))`). 
This constitutes the baseline above which predictive modeling is interesting.

### Recursive Feature elimination
```{r rfe wrapper, message=FALSE, paged.print=FALSE}
# Wrapper for the RFE algorithm
rfe_drug <- function(df, outcomeName){ 
  # Remove the id column
  df <- df %>% select(-Id) 

  #Make the Used class a factor
  df$Used <- factor(df$Used,
                     levels = c(0, 1), 
                     labels = c("0", "1"))

  # RFE controls
  control <- rfeControl(functions = rfFuncs,
                        method = "repeatedcv",
                        repeats = 3, # Change to 10 for final run
                        verbose = FALSE)
  # Exclude Class from the list of predictors
  predictors <- names(df)[!names(df) %in% outcomeName]

  # caret RFE Call
  pred_Profile <- rfe(df[ ,predictors], 
                      unlist(df[ ,outcomeName]), 
                      rfeControl = control)
  return(pred_Profile)
}
```

For RFE as well as subsequent modeling, we use the k-fold cross validation method which involves splitting the dataset into k subsets. The algorithm holds aside one of the subsets while the model is trained on the others. This process is repeated a predetermined number of times and the overall accuracy estimate is provided.

```{r rfe call, message=FALSE, paged.print=FALSE}
# outcomeName <- "Used"
# set.seed(5)
# rfe_Profile <- rfe_drug(df.train, outcomeName)
# #rfe_Profile
# predictors <- predictors(rfe_Profile)
# imp <- varImp(rfe_Profile, scale = TRUE)
```

```{r rfe profile plot, message=FALSE, paged.print=FALSE}
plot.profile.rfe
```

```{r rfe importance plot, message=FALSE, paged.print=FALSE}
plot.importance.rfe
```

The comparative analysis of the contribution of each factor agrees by and large with that of the density distribution plots: among the personality trait tests, N-score and E-score contribute the least while seeking sensation and O-score contribute the most. However, the impulsivity doesn't seem to be as strong a factor as one might have expected from the density plots.

```{r training parameters, message=FALSE, paged.print=FALSE}
# Training parameters
fitControl <- trainControl( 
  method = "repeatedcv", # Repeated k-fold Cross-Validation
  number = 3, # 5 for debugging CHANGE TO 10-fold CV
  repeats = 3, # 5 for debugging CHANGE TO 10 repeats
  allowParallel = TRUE,
  verbose = FALSE
)
```

### Generalized linear model
```{r glm, message=FALSE, warning=FALSE, paged.print=FALSE}
plot.varImp.glm
```
### Generalized linear model with penalized maximum likelihood
```{r glmnet , message=FALSE, warning=FALSE, paged.print=FALSE}
# GLMnet without parameter tuning
plot.varImp.glmnet.base

# GLMnet with parameter tuning
plot.varImp.glmnet
```

### Decision trees
```{r decision trees, message=FALSE, warning=FALSE, paged.print=FALSE}
# RPART Without parameter tuning
plot.varImp.rpart.base
# RPART With parameter tuning 
plot.varImp.rpart
```

### Random forest
```{r random forest, message=FALSE, warning=FALSE, paged.print=FALSE}
# RF without parameter tuning
plot.varImp.rf.base

# RF with parameter tuning
plot.varImp.rf
```

### Stochastic Gradient Boosting 
```{r gbm, message=FALSE, warning=FALSE, paged.print=FALSE}
# GBM without parameter tuning
plot.varImp.gbm.base

# GBM with parameter tuning
plot.varImp.gbm
```

### Neural network
```{r nnet base, message=FALSE, warning=FALSE, paged.print=FALSE}
# NNET without parameter tuning
plot.varImp.nnet.base

# NNET with parameter tuning
plot.varImp.nnet
```

### Model comparisons
```{r variable importance plots, message=FALSE, paged.print=FALSE}
#Variable importance
grid.arrange(plot.varImp.glm, plot.varImp.glmnet,
             plot.varImp.rpart, plot.varImp.rf,
             plot.varImp.gbm, plot.varImp.nnet,
             nrow = 2,
             top = "Model predictor importance",
             left = "Predictor",
             bottom = "Importance"
)
```

```{r model comparisons, message=FALSE, paged.print=FALSE}
plot.model.fit
```

# Results

GBM: Little importance is given to the education variable.
Improvement analysis -> what did the use of ML contribute

# Conclusion

Scope -> having used once in lifetime may not be telling. Could be interesting to bin never with over ten years ago ?